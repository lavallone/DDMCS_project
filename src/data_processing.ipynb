{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  âš™ï¸ Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘€ View Data\n",
    "We scraped all the data from ***Facebook*** and ***Instagram*** thanks to **CrowdTangle**, a social media analytics tool by Meta. The first thing to do in this cases is always ''look at the data!''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping**\n",
    "\n",
    "We wanted to analyze online conversations about two new emerging technologies: **ChatGPT** and **Apple Vision Pro**. The first technology breakthrough was analyzed in two different temporal windows: when ChatGPT 3.5 was launched (*30 Nov 2022*) and when ChatGPT 4 was launched (*14 Mar 2023*). Instead, Apple Vision Pro became available for purchase on *2 Feb 2024*.\n",
    "\n",
    "* For all the three events, we scraped data within a range of **3 months**. Trying to have better and more precise results, the starting point was the target date anticipated by 5 days (e.g. if the target date is *30 Nov 2022*, we scrape data starting from *25 Nov 2022* to *25 Feb 2023*).\n",
    "\n",
    "* In order to get the data the used *keywords* were {\"openai\", \"chatgpt\", \"llm\", \"gpt, \"gpt-3.5\" and \"gpt 3.5\"} for ChatGPT 3.5, {\"openai\", \"chatgpt\", \"llm\", \"gpt, \"gpt-4\" , \"gpt 4\"} for ChatGPT 4 and {\"apple vision pro\", \"vision pro\"} for Apple Vision Pro.\n",
    "\n",
    "> We selected only English posts and for Facebook, we filter out posts from Facebook groups by keeping only the ones for Facebook pages (scaling down from 1M to 100K). <br> All the scraped data are in *data/raw* folder!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb_gpt3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1456809/2418632965.py:10: DtypeWarning: Columns (13,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num_posts = len( pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\") )\n",
      "/tmp/ipykernel_1456809/2418632965.py:11: DtypeWarning: Columns (13,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num_users = pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\")[author_id].nunique()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 75811.\n",
      "Number of users: 24608.\n",
      "fb_gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1456809/2418632965.py:10: DtypeWarning: Columns (13,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num_posts = len( pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\") )\n",
      "/tmp/ipykernel_1456809/2418632965.py:11: DtypeWarning: Columns (13,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num_users = pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\")[author_id].nunique()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 89348.\n",
      "Number of users: 25329.\n",
      "fb_apple\n",
      "Number of posts: 8668.\n",
      "Number of users: 3178.\n",
      "ig_gpt3\n",
      "Number of posts: 11071.\n",
      "Number of users: 5409.\n",
      "ig_gpt4\n",
      "Number of posts: 24718.\n",
      "Number of users: 10303.\n",
      "ig_apple\n",
      "Number of posts: 2858.\n",
      "Number of users: 1997.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# see how many rows for each .csv files\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    for topic in topics:\n",
    "        print(f\"{platform}_{topic}\")\n",
    "        author_id = \"Facebook Id\" if platform == \"fb\" else \"User Name\"\n",
    "        num_posts = len( pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\") )\n",
    "        num_users = pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\")[author_id].nunique()\n",
    "        print(f\"Number of posts: {num_posts}.\")\n",
    "        print(f\"Number of users: {num_users}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAW DATA**\n",
    "\n",
    "Number of posts\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>75811</center></td>\n",
    "    <td><center>89348</center></td>\n",
    "    <td><center>8668</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>11071</center></td>\n",
    "    <td><center>24718</center></td>\n",
    "    <td><center>2858</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Number of users\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>24608</center></td>\n",
    "    <td><center>25329</center></td>\n",
    "    <td><center>3178</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>5409</center></td>\n",
    "    <td><center>10303</center></td>\n",
    "    <td><center>1997</center></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Facebook have more fields than data from Instagram, but are quite similar in the structure. The most substantial difference between the two platforms resides in the type of *interactions* that users can have. In Instagram there are only *likes* and *comments*. In Facebook we have, in addition to those, *shares* and *reactions* (LOVE, HAHA, WOW, SAD, ANGRY and CARE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we describe the carried out steps for **data cleaning**:\n",
    "* Dropping duplicates rows or rows where essential field have no information.\n",
    "* A further check of data range for each conversations topic.\n",
    "* Merging all text fields into a single one and start the *cleaning text* phase.\n",
    "    * In this phase are involved basic cleaning operations such as removing dirty strings, URLs, emojis and text lemmatization. <br> We decided to kep it as simple as possible because the model employed for sentiment/emotion analysis were finetuned on Twitter data, hence we needed to keep the social media natura as intact as possible! \n",
    "* As final steps there are: the language filtering operation where we keep only english posts and, only for facebook data a particular, attention to filter spam posts.\n",
    "\n",
    "> All the cleaned data are in *data/clean* folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define platforms and topics for the analysis\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "# we select and rename in a unified manner columns of data frame\n",
    "COLUMN_MAPPING = {\n",
    "    \"ig\": {\n",
    "        \"User Name\": \"author_id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Description\": \"text_1\",\n",
    "        \"Image Text\": \"text_2\",\n",
    "    },\n",
    "    \"fb\": {\n",
    "        \"Facebook Id\": \"author_id\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Message\": \"text_1\",\n",
    "        \"Description\": \"text_2\",\n",
    "        \"Link Text\": \"text_3\",\n",
    "        \"Love\" : \"Love\", \"Wow\":\"Wow\", \"Haha\":\"Haha\", \"Sad\":\"Sad\", \"Angry\":\"Angry\", \"Care\":\"Care\" # reactions\n",
    "    }, \n",
    "}\n",
    "# we define the temporal windows from which we want to extract conversations\n",
    "DATE_RANGE = {\n",
    "    \"gpt3\": {\"start\": \"2022-11-25\", \"end\": \"2023-02-25\"},\n",
    "    \"gpt4\": {\"start\": \"2023-03-09\", \"end\": \"2023-06-09\"},\n",
    "    \"apple\": {\"start\": \"2024-01-28\", \"end\": \"2024-04-28\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import merge_text, remove_facebook_spam, convert_date, filter_language, clean_text\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(raw_file_path, platform):\n",
    "    \"\"\"Load data from a given file path.\"\"\"\n",
    "    column_mapping = COLUMN_MAPPING[platform]\n",
    "    data = pd.read_csv(raw_file_path, usecols=column_mapping.keys(), low_memory=False)\n",
    "    data = data.rename(columns=column_mapping)\n",
    "    print(f\"Successfully loaded {platform} data from {raw_file_path}\")\n",
    "    return data\n",
    "\n",
    "def clean_data(data, topic):\n",
    "    \"\"\"Transform the raw data.\"\"\"\n",
    "    \n",
    "    # drop nan values for id, author_id, and date\n",
    "    len_data = len(data)\n",
    "    data = data.dropna(subset=[\"id\", \"author_id\", \"date\"]).reset_index(drop=True)\n",
    "    print(f\"After dropping nan values we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # drop duplicates\n",
    "    len_data = len(data)\n",
    "    data = data.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "    print(f\"After dropping duplicates we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # select time range (if the scraping worked well, no data should be dropped)\n",
    "    len_data = len(data)\n",
    "    data[\"date\"] = data[\"date\"].apply(convert_date)\n",
    "    data = data.dropna(subset=[\"date\"])\n",
    "    data = data.sort_values(by=[\"date\"])\n",
    "    start_date = convert_date(DATE_RANGE[topic][\"start\"])\n",
    "    end_date = convert_date(DATE_RANGE[topic][\"end\"])\n",
    "    data = data[(data[\"date\"] >= start_date) & (data[\"date\"] <= end_date)]\n",
    "    print(f\"Min date: {data['date'].min()}\")\n",
    "    print(f\"Max date: {data['date'].max()}\")\n",
    "    print(f\"After selecting time range we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # merge text if there are multiple text fields (our case)\n",
    "    data = merge_text(data)\n",
    "    # cleaning text (not too aggressive)\n",
    "    data.dropna(subset=[\"text\"], inplace=True)\n",
    "    data[\"text\"] = data[\"text\"].astype(str)\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    data[\"clean_text\"] = data[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "    data.dropna(subset=[\"clean_text\"], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # language filtering\n",
    "    data = filter_language(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cleaning process\n",
    "for topic in topics:\n",
    "    for platform in platforms:\n",
    "        data = load_data(raw_file_path=f\"../data/raw/{platform}_{topic}.csv\", platform=platform)\n",
    "        data = clean_data(data=data, topic=topic)\n",
    "        if platform == \"fb\":\n",
    "            len_data = len(data)\n",
    "            data = remove_facebook_spam(data)\n",
    "            print(f\"After spam detection we eliminated {len_data-len(data)} entries.\")\n",
    "        data.to_csv(f\"../data/clean/{platform}_{topic}.csv\", index=False)\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb_gpt3\n",
      "Number of posts: 72862.\n",
      "Number of users: 23502.\n",
      "fb_gpt4\n",
      "Number of posts: 85498.\n",
      "Number of users: 24526.\n",
      "fb_apple\n",
      "Number of posts: 8314.\n",
      "Number of users: 3014.\n",
      "ig_gpt3\n",
      "Number of posts: 9738.\n",
      "Number of users: 4886.\n",
      "ig_gpt4\n",
      "Number of posts: 21890.\n",
      "Number of users: 9203.\n",
      "ig_apple\n",
      "Number of posts: 2459.\n",
      "Number of users: 1700.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# see how many rows for each .csv files\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    for topic in topics:\n",
    "        print(f\"{platform}_{topic}\")\n",
    "        num_posts = len( pd.read_csv(f\"../data/clean/{platform}_{topic}.csv\") )\n",
    "        num_users = pd.read_csv(f\"../data/clean/{platform}_{topic}.csv\")[\"author_id\"].nunique()\n",
    "        print(f\"Number of posts: {num_posts}.\")\n",
    "        print(f\"Number of users: {num_users}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEAN DATA**\n",
    "\n",
    "Number of posts\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>75811 - <b>72862</b> (2949 less)</center></td>\n",
    "    <td><center>89348 - <b>85498</b> (3850 less)</center></td>\n",
    "    <td><center>8668 - <b>8314</b> (354 less)</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>11071 - <b>9738</b> (1333 less)</center></td>\n",
    "    <td><center>24718 - <b>21880</b> (2828 less)</center></td>\n",
    "    <td><center>2858 - <b>2459</b> (399 less)</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Number of users\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>24608 - <b>23502</b> (1106 less)</center></td>\n",
    "    <td><center>25329 - <b>24526</b> (803 less)</center></td>\n",
    "    <td><center>3178 - <b>3014</b> (164 less)</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>5409 - <b>4886</b> (523 less)</center></td>\n",
    "    <td><center>10303 - <b>9203</b> (1100 less)</center></td>\n",
    "    <td><center>1997 - <b>1700</b> (297 less)</center></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â›” Creation of training dataset for Sentiment/Emotion analysis with LLMs \n",
    "(not used in practice at the end but interesting to keep it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "sentiment_prompt = \"\"\"What is the sentiment of this text? \\nText: {text} \\nOptions: [ \"strongly negative\", \"negative\", \"negative or neutral\", \"positive\", \"strongly positive\"] \\nAnswer: {answer}\"\"\"\n",
    "emotion_prompt = \"\"\"Which emotions from the options below are expressed in the following text? \\nText: {text} \\nOptions: [ \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\" ] \\nAnswer: {answer}\"\"\"\n",
    "\n",
    "label2emotion = {0 : \"anger\", 1 : \"anticipation\", 2 : \"disgust\", 3 : \"fear\", \n",
    "                 4 : \"joy\", 5 : \"love\", 6 : \"optimism\", 7 : \"pessimism\", 8 : \"sadness\", 9 : \"surprise\", 10 : \"trust\"}\n",
    "label2sentiment = {0 : \"strongly negative\", 1 : \"negative\", 2 : \"negative or neutral\", 3 : \"positive\", 4 : \"strongly positive\"}\n",
    "\n",
    "def manipulate_emotion_labels(label):\n",
    "    ris = []\n",
    "    for i,e in enumerate(label):\n",
    "        if e == 1:\n",
    "            ris.append(label2emotion[i])\n",
    "    return \", \".join(ris)\n",
    "\n",
    "\n",
    "def generate_finetuning_dataset(dataset_type, texts, labels):\n",
    "\n",
    "    json_data = []\n",
    "    with open(f\"training_{dataset_type}.json\", \"w\") as fw_json:\n",
    "        for instance_data, instance_gold in tqdm(zip(texts, labels), total=len(labels)):\n",
    "            if dataset_type==\"emotion\":\n",
    "                answer = manipulate_emotion_labels(instance_gold)\n",
    "            else:\n",
    "                answer = label2sentiment[instance_gold]\n",
    "            \n",
    "            prompt_template = emotion_prompt if dataset_type==\"emotion\" else sentiment_prompt\n",
    "            prompt = prompt_template.format(\n",
    "                    text=instance_data,\n",
    "                    answer=answer)\n",
    "            json_elem = {\"prompt\":prompt}\n",
    "            json_data.append(json_elem)\n",
    "        json.dump(json_data, fw_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6838/6838 [00:00<00:00, 456858.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26632/26632 [00:00<00:00, 830355.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# EMOTION \n",
    "emotion_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_emotion\")\n",
    "\n",
    "emotion_train_dataset = emotion_dataset['train']\n",
    "emotion_train_texts = emotion_train_dataset['text']\n",
    "emotion_train_labels = emotion_train_dataset['gold_label_list']\n",
    "\n",
    "generate_finetuning_dataset(\"emotion\", emotion_train_texts, emotion_train_labels)\n",
    "\n",
    "# SENTIMENT\n",
    "sentiment_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_sentiment\")\n",
    "\n",
    "sentiment_train_dataset = sentiment_dataset['train']\n",
    "sentiment_train_texts = sentiment_train_dataset['text']\n",
    "sentiment_train_labels = sentiment_train_dataset['gold_label']\n",
    "\n",
    "generate_finetuning_dataset(\"sentiment\", sentiment_train_texts, sentiment_train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sappia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
