{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ⚙️ Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👀 View Data\n",
    "We scraped all the data from ***Facebook*** and ***Instagram*** thanks to **CrowdTangle**, a social media analytics tool by Meta. The first thing to do in this cases is always ''look at the data!''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping**\n",
    "\n",
    "We wanted to analyze online conversations about two new emerging technologies: **ChatGPT** and **Apple Vision Pro**. The first technology breakthrough was analyzed in two different temporal windows: when ChatGPT 3.5 was launched (*30 Nov 2022*) and when ChatGPT 4 was launched (*14 Mar 2023*). Instead, Apple Vision Pro became available for purchase on *2 Feb 2024*.\n",
    "\n",
    "* For all the three events, we scraped data within a range of **3 months**. Trying to have better and more precise results, the starting point was the target date anticipated by 5 days (e.g. if the target date is *30 Nov 2022*, we scrape data starting from *25 Nov 2022* to *25 Feb 2023*).\n",
    "\n",
    "* In order to get the data the used *keywords* were {\"openai\", \"chatgpt\", \"llm\", \"gpt, \"gpt-3.5\" and \"gpt 3.5\"} for ChatGPT 3.5, {\"openai\", \"chatgpt\", \"llm\", \"gpt, \"gpt-4\" , \"gpt 4\"} for ChatGPT 4 and {\"apple vision pro\", \"vision pro\"} for Apple Vision Pro.\n",
    "\n",
    "> We selected only English posts and for Facebook, we filter out posts from Facebook groups by keeping only the ones for Facebook pages (scaling down from 1M to 100K). <br> All the scraped data are in *data/raw* folder!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# see how many rows for each .csv files\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    for topic in topics:\n",
    "        print(f\"{platform}_{topic}\")\n",
    "        print( len( pd.read_csv(f\"../data/raw/{platform}_{topic}.csv\") ) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAW DATA**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>75811</center></td>\n",
    "    <td><center>89348</center></td>\n",
    "    <td><center>8668</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>11071</center></td>\n",
    "    <td><center>24718</center></td>\n",
    "    <td><center>2858</center></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Facebook have more fields than data from Instagram, but are quite similar in the structure. The most substantial difference between the two platforms resides in the type of *interactions* that users can have. In Instagram there are only *likes* and *comments*. In Facebook we have, in addition to those, *shares* and *reactions* (LOVE, HAHA, WOW, SAD, ANGRY and CARE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here you decribe the design coices for the cleaning \n",
    "4. make sure you remove posts by account  Crypto PH from chatgpt fb posts. that's a spam account which used chatgpt related tags to get attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define platforms and topics for the analysis\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "# we select and rename in a unified manner columns of data frame\n",
    "COLUMN_MAPPING = {\n",
    "    \"ig\": {\n",
    "        \"User Name\": \"author_id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Description\": \"text_1\",\n",
    "        \"Image Text\": \"text_2\",\n",
    "    },\n",
    "    \"fb\": {\n",
    "        \"Facebook Id\": \"author_id\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Message\": \"text_1\",\n",
    "        \"Description\": \"text_2\",\n",
    "        \"Link Text\": \"text_3\",\n",
    "        \"Love\" : \"Love\", \"Wow\":\"Wow\",\"Haha\":\"Haha\", \"Sad\":\"Sad\", \"Angry\":\"Angry\", \"Care\":\"Care\" # reactions\n",
    "    }, \n",
    "}\n",
    "# we define the temporal windows from which we want to extract conversations\n",
    "DATE_RANGE = {\n",
    "    \"gpt3\": {\"start\": \"2022-11-25\", \"end\": \"2023-02-25\"},\n",
    "    \"gpt4\": {\"start\": \"2023-03-09\", \"end\": \"2023-06-09\"},\n",
    "    \"apple\": {\"start\": \"2024-01-28\", \"end\": \"2024-04-28\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import merge_text, remove_facebook_spam, convert_date, filter_language, clean_text\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(raw_file_path, platform):\n",
    "    \"\"\"Load data from a given file path.\"\"\"\n",
    "    column_mapping = COLUMN_MAPPING[platform]\n",
    "    data = pd.read_csv(raw_file_path, usecols=column_mapping.keys(), low_memory=False)\n",
    "    data = data.rename(columns=column_mapping)\n",
    "    print(f\"Successfully loaded {platform} data from {raw_file_path}\")\n",
    "    return data\n",
    "\n",
    "def clean_data(data, topic):\n",
    "    \"\"\"Transform the raw data.\"\"\"\n",
    "    \n",
    "    # drop nan values for id, author_id, and date\n",
    "    len_data = len(data)\n",
    "    data = data.dropna(subset=[\"id\", \"author_id\", \"date\"]).reset_index(drop=True)\n",
    "    print(f\"After dropping nan values we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # drop duplicates\n",
    "    len_data = len(data)\n",
    "    data = data.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "    print(f\"After dropping duplicates we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # select time range (if the scraping worked well, no data should be dropped)\n",
    "    len_data = len(data)\n",
    "    data[\"date\"] = data[\"date\"].apply(convert_date)\n",
    "    data = data.dropna(subset=[\"date\"])\n",
    "    data = data.sort_values(by=[\"date\"])\n",
    "    start_date = convert_date(DATE_RANGE[topic][\"start\"])\n",
    "    end_date = convert_date(DATE_RANGE[topic][\"end\"])\n",
    "    data = data[(data[\"date\"] >= start_date) & (data[\"date\"] <= end_date)]\n",
    "    print(f\"Min date: {data['date'].min()}\")\n",
    "    print(f\"Max date: {data['date'].max()}\")\n",
    "    print(f\"After selecting time range we eliminated {len_data-len(data)} entries.\")\n",
    "\n",
    "    # merge text if there are multiple text fields (our case)\n",
    "    data = merge_text(data)\n",
    "    # cleaning text (not too aggressive)\n",
    "    data.dropna(subset=[\"text\"], inplace=True)\n",
    "    data[\"text\"] = data[\"text\"].astype(str)\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    data[\"clean_text\"] = data[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "    data.dropna(subset=[\"clean_text\"], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # language filtering\n",
    "    data = filter_language(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cleaning process\n",
    "for topic in topics:\n",
    "    for platform in platforms:\n",
    "        data = load_data(raw_file_path=f\"../data/raw/{platform}_{topic}.csv\", platform=platform)\n",
    "        data = clean_data(data=data, topic=topic)\n",
    "        if platform == \"fb\":\n",
    "            len_data = len(data)\n",
    "            data = remove_facebook_spam(data)\n",
    "            print(f\"After spam detection we eliminated {len_data-len(data)} entries.\")\n",
    "        data.to_csv(f\"../data/clean/{platform}_{topic}.csv\", index=False)\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb_gpt3\n",
      "72862\n",
      "fb_gpt4\n",
      "85498\n",
      "fb_apple\n",
      "8314\n",
      "ig_gpt3\n",
      "9738\n",
      "ig_gpt4\n",
      "21890\n",
      "ig_apple\n",
      "2459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# see how many rows for each .csv files\n",
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    for topic in topics:\n",
    "        print(f\"{platform}_{topic}\")\n",
    "        print( len( pd.read_csv(f\"../data/clean/{platform}_{topic}.csv\") ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEAN DATA**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><center> </center></td>\n",
    "    <th><center>GPT-3.5</center></th>\n",
    "    <th><center>GPT-4</center></th>\n",
    "    <th><center>Apple Vision Pro</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Facebook</center></th>\n",
    "    <td><center>75811 - <b>72862</b> (2949 less)</center></td>\n",
    "    <td><center>89348 - <b>85498</b> (3850 less)</center></td>\n",
    "    <td><center>8668 - <b>8314</b> (354 less)</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>Instagram</center></th>\n",
    "    <td><center>11071 - <b>9738</b> (1333 less)</center></td>\n",
    "    <td><center>24718 - <b>21890</b> (2828 less)</center></td>\n",
    "    <td><center>2858 - <b>2459</b> (399 less)</center></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⛔ Creation of training dataset for Sentiment/Emotion analysis with LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_emotion\")\n",
    "sentiment_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOTION\n",
    "\n",
    "label2emotion = {0 : \"anger\", 1 : \"anticipation\", 2 : \"disgust\", 3 : \"fear\", \n",
    "                 4 : \"joy\", 5 : \"love\", 6 : \"optimism\", 7 : \"pessimism\", 8 : \"sadness\", 9 : \"surprise\", 10 : \"trust\"}\n",
    "\n",
    "def manipulate_emotion_labels(label):\n",
    "    ris = []\n",
    "    for i,e in enumerate(label):\n",
    "        if e == 1:\n",
    "            ris.append(label2emotion[i])\n",
    "    return ris\n",
    "\n",
    "emotion_train_dataset = emotion_dataset['train']\n",
    "emotion_test_dataset = emotion_dataset['test']\n",
    "emotion_validation_dataset = emotion_dataset['validation']\n",
    "\n",
    "emotion_train_texts = emotion_train_dataset['text']\n",
    "emotion_train_labels = emotion_train_dataset['gold_label_list']\n",
    "\n",
    "for i in range(15,20):\n",
    "    print(emotion_train_texts[i])\n",
    "    print(manipulate_emotion_labels(emotion_train_labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "sentiment_prompt = \"\"\"What is the sentiment of this text? \\nText: {text} \\nOptions: [ \"strongly negative\", \"negative\", \"negative or neutral\", \"positive\", \"strongly positive\"] \\nAnswer: {answer}\"\"\"\n",
    "emotion_prompt = \"\"\"Which emotions from the options below are expressed in the following text? \\nText: {text} \\nOptions: [ \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\" ] \\nAnswer: {answer}\"\"\"\n",
    "\n",
    "label2emotion = {0 : \"anger\", 1 : \"anticipation\", 2 : \"disgust\", 3 : \"fear\", \n",
    "                 4 : \"joy\", 5 : \"love\", 6 : \"optimism\", 7 : \"pessimism\", 8 : \"sadness\", 9 : \"surprise\", 10 : \"trust\"}\n",
    "label2sentiment = {0 : \"strongly negative\", 1 : \"negative\", 2 : \"negative or neutral\", 3 : \"positive\", 4 : \"strongly positive\"}\n",
    "\n",
    "def manipulate_emotion_labels(label):\n",
    "    ris = []\n",
    "    for i,e in enumerate(label):\n",
    "        if e == 1:\n",
    "            ris.append(label2emotion[i])\n",
    "    return \", \".join(ris)\n",
    "\n",
    "\n",
    "def generate_finetuning_dataset(dataset_type, texts, labels):\n",
    "\n",
    "    json_data = []\n",
    "    with open(f\"training_{dataset_type}.json\", \"w\") as fw_json:\n",
    "        for instance_data, instance_gold in tqdm(zip(texts, labels), total=len(labels)):\n",
    "            if dataset_type==\"emotion\":\n",
    "                answer = manipulate_emotion_labels(instance_gold)\n",
    "            else:\n",
    "                answer = label2sentiment[instance_gold]\n",
    "            \n",
    "            prompt_template = emotion_prompt if dataset_type==\"emotion\" else sentiment_prompt\n",
    "            prompt = prompt_template.format(\n",
    "                    text=instance_data,\n",
    "                    answer=answer)\n",
    "            json_elem = {\"prompt\":prompt}\n",
    "            json_data.append(json_elem)\n",
    "        json.dump(json_data, fw_json, indent=4)\n",
    "        \n",
    "generate_finetuning_dataset(\"emotion\", emotion_train_texts, emotion_train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sappia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
