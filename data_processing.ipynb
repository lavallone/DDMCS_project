{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ‚öôÔ∏è Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ View Data\n",
    "We scraped all the data from ***Facebook*** and ***Instagram*** thanks to **CrowdTangle**, a social media analytics tool by Meta. The first thing to do in this cases is always ''look at the data!''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can explain the structure of the data for each platform and which field we keep in the cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING TRAINING DATA FOR SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_emotion\")\n",
    "sentiment_dataset = load_dataset(\"cardiffnlp/super_tweeteval\", \"tweet_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People you need to look up the definition of protest. What you are doing is not protesting is called vandalism. #angry #stop\n",
      "['anger', 'disgust', 'sadness']\n",
      "@user Look at those teef! #growl\n",
      "['anger', 'disgust', 'fear']\n",
      "Star trek online has a update to download oh fuming yay\n",
      "['anger', 'disgust', 'joy', 'sadness']\n",
      "The bitter the battle, the sweeter the victory...\n",
      "['joy', 'optimism']\n",
      "i cant stop. i finished - dejected. luckily no one is in the bathroom. so i go to a stall and wait until my pants are dry.\n",
      "['anger', 'disgust', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "# EMOTION\n",
    "\n",
    "label2emotion = {0 : \"anger\", 1 : \"anticipation\", 2 : \"disgust\", 3 : \"fear\", \n",
    "                 4 : \"joy\", 5 : \"love\", 6 : \"optimism\", 7 : \"pessimism\", 8 : \"sadness\", 9 : \"surprise\", 10 : \"trust\"}\n",
    "\n",
    "def manipulate_emotion_labels(label):\n",
    "    ris = []\n",
    "    for i,e in enumerate(label):\n",
    "        if e == 1:\n",
    "            ris.append(label2emotion[i])\n",
    "    return ris\n",
    "\n",
    "emotion_train_dataset = emotion_dataset['train']\n",
    "emotion_test_dataset = emotion_dataset['test']\n",
    "emotion_validation_dataset = emotion_dataset['validation']\n",
    "\n",
    "emotion_train_texts = emotion_train_dataset['text']\n",
    "emotion_train_labels = emotion_train_dataset['gold_label_list']\n",
    "\n",
    "for i in range(15,20):\n",
    "    print(emotion_train_texts[i])\n",
    "    print(manipulate_emotion_labels(emotion_train_labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'text', 'target'],\n",
      "        num_rows: 26632\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'text', 'target'],\n",
      "        num_rows: 12379\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['gold_label', 'text', 'target'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "[\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\", \"@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\", \"I may be ignorant on this issue but... should we celebrate @user parental leave changes? Doesn't the gender divide suggest... (1/2)\", 'Thanks to @user I just may be switching over to @user', 'If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft']\n",
      "[1, 0, 1, 1, 2]\n",
      "['@microsoft', '@microsoft', '@microsoft', '@microsoft', '@microsoft']\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_dataset)\n",
    "sentiment_train_dataset = sentiment_dataset['train']\n",
    "sentiment_test_dataset = sentiment_dataset['test']\n",
    "sentiment_validation_dataset = sentiment_dataset['validation']\n",
    "\n",
    "sentiment_train_texts = sentiment_train_dataset['text']\n",
    "sentiment_train_labels = sentiment_train_dataset['gold_label']\n",
    "#sentiment_train_targets = sentiment_train_dataset['target']\n",
    "\n",
    "print(sentiment_train_texts[:5])\n",
    "print(sentiment_train_labels[:5])\n",
    "#print(sentiment_train_targets[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6838/6838 [00:00<00:00, 517860.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "sentiment_prompt = \"\"\"What is the sentiment of this text? \\nText: {text} \\nOptions: [ \"strongly negative\", \"negative\", \"negative or neutral\", \"positive\", \"strongly positive\"] \\nAnswer: {answer}\"\"\"\n",
    "emotion_prompt = \"\"\"Which emotions from the options below are expressed in the following text? \\nText: {text} \\nOptions: [ \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\" ] \\nAnswer: {answer}\"\"\"\n",
    "\n",
    "label2emotion = {0 : \"anger\", 1 : \"anticipation\", 2 : \"disgust\", 3 : \"fear\", \n",
    "                 4 : \"joy\", 5 : \"love\", 6 : \"optimism\", 7 : \"pessimism\", 8 : \"sadness\", 9 : \"surprise\", 10 : \"trust\"}\n",
    "label2sentiment = {0 : \"strongly negative\", 1 : \"negative\", 2 : \"negative or neutral\", 3 : \"positive\", 4 : \"strongly positive\"}\n",
    "\n",
    "def manipulate_emotion_labels(label):\n",
    "    ris = []\n",
    "    for i,e in enumerate(label):\n",
    "        if e == 1:\n",
    "            ris.append(label2emotion[i])\n",
    "    return \", \".join(ris)\n",
    "\n",
    "\n",
    "def generate_finetuning_dataset(dataset_type, texts, labels):\n",
    "\n",
    "    json_data = []\n",
    "    with open(f\"training_{dataset_type}.json\", \"w\") as fw_json:\n",
    "        for instance_data, instance_gold in tqdm(zip(texts, labels), total=len(labels)):\n",
    "            if dataset_type==\"emotion\":\n",
    "                answer = manipulate_emotion_labels(instance_gold)\n",
    "            else:\n",
    "                answer = label2sentiment[instance_gold]\n",
    "            \n",
    "            prompt_template = emotion_prompt if dataset_type==\"emotion\" else sentiment_prompt\n",
    "            prompt = prompt_template.format(\n",
    "                    text=instance_data,\n",
    "                    answer=answer)\n",
    "            json_elem = {\"prompt\":prompt}\n",
    "            json_data.append(json_elem)\n",
    "        json.dump(json_data, fw_json, indent=4)\n",
    "        \n",
    "generate_finetuning_dataset(\"emotion\", emotion_train_texts, emotion_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# mi sa che non serve nemmeno questa\n",
    "def convert_date(date_str: str) -> datetime.date:\n",
    "    # Cleaning date string: removing timezone, we are interested in the date component.\n",
    "    cleaned_str = re.sub(r\"[A-Z]|T|Z\", \" \", date_str).strip()\n",
    "\n",
    "    try:\n",
    "        dt = pd.to_datetime(cleaned_str, errors=\"coerce\")\n",
    "        return dt.date()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def merge_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_columns = [col for col in data.columns if \"text\" in col.lower()]\n",
    "\n",
    "    def process_text_cols(text_cols: pd.Series) -> str:\n",
    "        text_values = [value for value in text_cols if pd.notna(value)]\n",
    "        return \" \".join(map(str, text_values))\n",
    "\n",
    "    if len(text_columns) > 1:\n",
    "        data[\"text\"] = data[text_columns].apply(process_text_cols, axis=1)\n",
    "        data = data.drop(columns=text_columns)\n",
    "    else:\n",
    "        data[\"text\"] = data[\"text\"].astype(str)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# IT'S NOT NEEDED ACCORDING TO ME, LET'S SEE\n",
    "def merge_interactions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    interaction_columns = [col for col in data.columns if \"interaction\" in col.lower()]\n",
    "\n",
    "    def process_interaction_cols(interaction_cols: pd.Series) -> pd.Series:\n",
    "        interaction_cols = interaction_cols.fillna(0)\n",
    "        interaction_cols = interaction_cols.astype(str).str.replace(\",\", \"\")\n",
    "        interaction_cols = interaction_cols.apply(\n",
    "            lambda x: pd.to_numeric(x, errors=\"coerce\")\n",
    "        )\n",
    "        return interaction_cols\n",
    "\n",
    "    processed_interaction_cols = data[interaction_columns].apply(\n",
    "        process_interaction_cols, axis=1\n",
    "    )\n",
    "\n",
    "    # If multiple interaction columns, sum them up\n",
    "    if len(interaction_columns) > 1:\n",
    "        data[\"interaction\"] = processed_interaction_cols.sum(axis=1)\n",
    "        data = data.drop(columns=interaction_columns)\n",
    "    else:\n",
    "        data[\"interaction\"] = processed_interaction_cols[interaction_columns[0]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_CONFIG = {\n",
    "    \"ig\": {\n",
    "        \"User Name\": \"author_id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Description\": \"text_1\",\n",
    "        \"Image Text\": \"text_2\",\n",
    "    },\n",
    "    \"fb\": {\n",
    "        \"Facebook Id\": \"author_id\",\n",
    "        \"Total Interactions\": \"interaction\",\n",
    "        \"URL\": \"id\",\n",
    "        \"Post Created Date\": \"date\",\n",
    "        \"Message\": \"text_1\",\n",
    "        \"Description\": \"text_2\",\n",
    "        \"Link Text\": \"text_3\",\n",
    "    }, \n",
    "}\n",
    "\n",
    "DATE_RANGE = {\n",
    "    \"gpt3\": {\"start\": \"2022-11-25\", \"end\": \"2023-02-25\"},\n",
    "    \"gpt4\": {\"start\": \"2023-03-09\", \"end\": \"2023-06-09\"},\n",
    "    \"apple\": {\"start\": \"2024-01-28\", \"end\": \"2024-04-28\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "#from .helper import (\n",
    "#    convert_date,\n",
    "#    merge_text,\n",
    "#    merge_interactions,\n",
    "#)\n",
    "\n",
    "\n",
    "class BaseDataLoader:\n",
    "    \"\"\"Base class for data loaders.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, topic: str, platform: str):\n",
    "        self.file_path = file_path # csv file path\n",
    "        self.topic = topic # gpt3, gpt4 or apple\n",
    "        self.platform = platform # ig or fb\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from a given file path.\"\"\"\n",
    "        column_mapping = COLUMN_CONFIG[self.platform]\n",
    "        try:\n",
    "            self.data = pd.read_csv(\n",
    "                self.file_path, usecols=column_mapping.keys(), low_memory=False\n",
    "            )\n",
    "            self.data = self.data.rename(columns=column_mapping)\n",
    "            print(\n",
    "                f\"Successfully loaded {self.platform} data from {self.file_path}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.platform} data: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "    def transform_data(self):\n",
    "        \"\"\"Transform the raw data.\"\"\"\n",
    "        \n",
    "        print(f\"Data shape before dropping nan values: {self.data.shape}\")\n",
    "        # drop nan values for id, author_id, and date\n",
    "        self.data = self.data.dropna(subset=[\"id\", \"author_id\", \"date\"]).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        print(f\"Data shape after dropping nan values: {self.data.shape}\")\n",
    "\n",
    "        print(f\"Data shape before dropping duplicates: {self.data.shape}\")\n",
    "        # drop duplicates\n",
    "        self.data = self.data.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "        print(f\"Data shape after dropping duplicates: {self.data.shape}\")\n",
    "\n",
    "        # convert date column (let's see how it convert it)\n",
    "        self.data[\"date\"] = self.data[\"date\"].apply(convert_date)\n",
    "        self.data = self.data.dropna(subset=[\"date\"])\n",
    "        self.data = self.data.sort_values(by=[\"date\"])\n",
    "\n",
    "        # select time range\n",
    "        print(DATE_RANGE[self.topic][\"start\"])\n",
    "        start_date = convert_date(DATE_RANGE[self.topic][\"start\"])\n",
    "        print(start_date)\n",
    "        end_date = convert_date(DATE_RANGE[self.topic][\"end\"])\n",
    "        self.data = self.data[\n",
    "            (self.data[\"date\"] >= start_date) & (self.data[\"date\"] <= end_date)\n",
    "        ] # check of time range but it should be okay\n",
    "        print(f\"Data shape after selecting time range: {self.data.shape}\")\n",
    "        print(f\"Min date: {self.data['date'].min()}\")\n",
    "        print(f\"Max date: {self.data['date'].max()}\")\n",
    "        print(\"Processed date column\")\n",
    "\n",
    "        # Process text columns\n",
    "        # Merge text columns\n",
    "        self.data = merge_text(self.data) #we merge text_1, text_2 etc.\n",
    "        print(\"Processed text columns\")\n",
    "\n",
    "        # Merge interaction columns\n",
    "        self.data = merge_interactions(self.data)\n",
    "        print(\"Processed interaction columns\")\n",
    "        \n",
    "    def remove_facebook_spam(self):\n",
    "        \"\"\"Detect and remove spam Facebook posts from the data.\"\"\"\n",
    "        if self.platform != \"facebook\":\n",
    "            return\n",
    "\n",
    "        print(\"Starting spam detection for Facebook posts.\")\n",
    "        self.data = self.data.copy()\n",
    "\n",
    "        # Initialize a new spam column with 0\n",
    "        self.data[\"spam\"] = 0\n",
    "\n",
    "        # Convert text column to string\n",
    "        self.data[\"text\"] = self.data[\"text\"].astype(str)\n",
    "\n",
    "        # Define spam patterns\n",
    "        spam_patterns = [\n",
    "            \"Video Funny Amazing #fyp #viral\",\n",
    "            \"#reeel #cr7# #chatgpt\",\n",
    "            \"#reels #chatgpt\",\n",
    "            \"https://www.facebook.com/100076267686928/posts/202421482310107\",\n",
    "        ]\n",
    "\n",
    "        # If a row's text contains any of the spam patterns, set spam = 1\n",
    "        for pattern in spam_patterns:\n",
    "            self.data.loc[\n",
    "                self.data[\"text\"].str.contains(pattern, case=False, na=False), \"spam\"\n",
    "            ] = 1\n",
    "\n",
    "        spam_counts = self.data[\"spam\"].value_counts()\n",
    "        print(\n",
    "            f\"Detected {spam_counts.get(1, 0)} spam posts and {spam_counts.get(0, 0)} non-spam posts.\"\n",
    "        )\n",
    "\n",
    "        # Filter out spam posts\n",
    "        self.data = self.data[self.data[\"spam\"] == 0]\n",
    "        self.data.drop(columns=[\"spam\"], inplace=True)\n",
    "\n",
    "        print(f\"Spam posts removed. Remaining posts count: {len(self.data)}.\")\n",
    "\n",
    "\n",
    "    def process_data(self):\n",
    "        # load data\n",
    "        self.load_data()\n",
    "        # transform data\n",
    "        self.transform_data()\n",
    "        if self.platform == \"fb\":\n",
    "            self.remove_facebook_spam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before dropping nan values: (75811, 7)\n",
      "Data shape after dropping nan values: (75810, 7)\n",
      "Data shape before dropping duplicates: (75810, 7)\n",
      "Data shape after dropping duplicates: (75810, 7)\n",
      "2022-11-25\n",
      "2022-11-25\n",
      "Data shape after selecting time range: (75482, 7)\n",
      "Min date: 2022-11-25\n",
      "Max date: 2023-02-23\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n",
      "Data shape before dropping nan values: (11071, 6)\n",
      "Data shape after dropping nan values: (11071, 6)\n",
      "Data shape before dropping duplicates: (11071, 6)\n",
      "Data shape after dropping duplicates: (10996, 6)\n",
      "2022-11-25\n",
      "2022-11-25\n",
      "Data shape after selecting time range: (10930, 6)\n",
      "Min date: 2022-11-25\n",
      "Max date: 2023-02-23\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n",
      "Data shape before dropping nan values: (89348, 7)\n",
      "Data shape after dropping nan values: (89348, 7)\n",
      "Data shape before dropping duplicates: (89348, 7)\n",
      "Data shape after dropping duplicates: (89348, 7)\n",
      "2023-03-09\n",
      "2023-03-09\n",
      "Data shape after selecting time range: (88660, 7)\n",
      "Min date: 2023-03-09\n",
      "Max date: 2023-06-09\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n",
      "Data shape before dropping nan values: (24718, 6)\n",
      "Data shape after dropping nan values: (24718, 6)\n",
      "Data shape before dropping duplicates: (24718, 6)\n",
      "Data shape after dropping duplicates: (24488, 6)\n",
      "2023-03-09\n",
      "2023-03-09\n",
      "Data shape after selecting time range: (24299, 6)\n",
      "Min date: 2023-03-09\n",
      "Max date: 2023-06-09\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n",
      "Data shape before dropping nan values: (8668, 7)\n",
      "Data shape after dropping nan values: (8668, 7)\n",
      "Data shape before dropping duplicates: (8668, 7)\n",
      "Data shape after dropping duplicates: (8668, 7)\n",
      "2024-01-28\n",
      "2024-01-28\n",
      "Data shape after selecting time range: (8630, 7)\n",
      "Min date: 2024-01-28\n",
      "Max date: 2024-04-28\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n",
      "Data shape before dropping nan values: (2858, 6)\n",
      "Data shape after dropping nan values: (2858, 6)\n",
      "Data shape before dropping duplicates: (2858, 6)\n",
      "Data shape after dropping duplicates: (2796, 6)\n",
      "2024-01-28\n",
      "2024-01-28\n",
      "Data shape after selecting time range: (2781, 6)\n",
      "Min date: 2024-01-28\n",
      "Max date: 2024-04-28\n",
      "Processed date column\n",
      "Processed text columns\n",
      "Processed interaction columns\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "platforms = [\"fb\", \"ig\"]\n",
    "topics = [\"gpt3\", \"gpt4\", \"apple\"]\n",
    "\n",
    "for topic in topics:\n",
    "    for platform in platforms:\n",
    "        data_loader = BaseDataLoader(f\"data/raw/{platform}_{topic}.csv\", topic, platform)\n",
    "        data_loader.process_data()\n",
    "        data_loader.data.to_csv(f\"data/cleaned/{platform}_{topic}.csv\", index=False)\n",
    "        print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sappia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
